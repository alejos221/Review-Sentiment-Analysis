{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;START this film was just brilliant casting lo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;START big hair big boobs bad music and a gian...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;START this has to be one of the worst films o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;START the &lt;UNK&gt; &lt;UNK&gt; at storytelling the tra...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;START worst mistake of my life br br i picked...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Sentiment\n",
       "0  <START this film was just brilliant casting lo...  positive\n",
       "1  <START big hair big boobs bad music and a gian...  negative\n",
       "2  <START this has to be one of the worst films o...  negative\n",
       "3  <START the <UNK> <UNK> at storytelling the tra...  positive\n",
       "4  <START worst mistake of my life br br i picked...  negative"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file\n",
    "data = pd.read_csv('imdb_reviews.csv')\n",
    "\n",
    "# Reserves data for train and testing (70:30)\n",
    "imdb_reviews = pd.DataFrame(np.array(data)[:17500], columns=('Reviews', 'Sentiment'))\n",
    "test_reviews = pd.DataFrame(np.array(data)[17500:], columns=('Reviews', 'Sentiment'))\n",
    "\n",
    "imdb_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dictionary of words with associated IDs using the datasets from keras\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Adding new mapping for exclusive/html words\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "word_index['to']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode review data using mapping\n",
    "def review_encoder(text):\n",
    "    arr = [word_index[word] for word in text]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = imdb_reviews['Reviews'], imdb_reviews['Sentiment']\n",
    "test_data, test_labels = test_reviews['Reviews'], test_reviews['Sentiment']\n",
    "\n",
    "\n",
    "# This separates each 'Review' into an array of words (in both the train and test data)\n",
    "# It excludes tokenizing the labels since we will handle that separately later\n",
    "train_data = train_data.apply(lambda review:review.split())\n",
    "test_data = test_data.apply(lambda review:review.split())\n",
    "\n",
    "# This applies the review encoder on each array of words (in both the train and test data)\n",
    "# This is so it encodes each word of that 'Review' row\n",
    "train_data = train_data.apply(review_encoder)\n",
    "test_data = test_data.apply(review_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode sentiment data using mapping\n",
    "def sentiment_encoder(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This applies the sentiment encoder on each label (in both train and test data)\n",
    "train_labels = train_labels.apply(sentiment_encoder)\n",
    "test_labels = test_labels.apply(sentiment_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the Reviews so that short Reviews have the same size (500)\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding='post', maxlen=500)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], padding='post', maxlen=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining/preparing neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Embedding(10000, 16, input_length=500),   # Word embedding layer\n",
    "                         keras.layers.GlobalAveragePooling1D(),                 # Global average pooling layer to avoid overfitting by minimizing parameters\n",
    "                         keras.layers.Dense(16, activation='relu'),             # ReLU dense layer\n",
    "                         keras.layers.Dense(1, activation='sigmoid')])           # Output layer using sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using adam optimizer, binary cross entropy loss, and accuracy for metrics\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5061 - loss: 0.6931 - val_accuracy: 0.5657 - val_loss: 0.6909\n",
      "Epoch 2/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5498 - loss: 0.6901 - val_accuracy: 0.5687 - val_loss: 0.6864\n",
      "Epoch 3/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5996 - loss: 0.6837 - val_accuracy: 0.5752 - val_loss: 0.6711\n",
      "Epoch 4/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6370 - loss: 0.6629 - val_accuracy: 0.6867 - val_loss: 0.6366\n",
      "Epoch 5/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6777 - loss: 0.6289 - val_accuracy: 0.7449 - val_loss: 0.6006\n",
      "Epoch 6/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7582 - loss: 0.5886 - val_accuracy: 0.7740 - val_loss: 0.5596\n",
      "Epoch 7/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7856 - loss: 0.5451 - val_accuracy: 0.7231 - val_loss: 0.5394\n",
      "Epoch 8/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7964 - loss: 0.5045 - val_accuracy: 0.7743 - val_loss: 0.4913\n",
      "Epoch 9/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8170 - loss: 0.4686 - val_accuracy: 0.7680 - val_loss: 0.4717\n",
      "Epoch 10/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8259 - loss: 0.4400 - val_accuracy: 0.8397 - val_loss: 0.4234\n",
      "Epoch 11/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8579 - loss: 0.4008 - val_accuracy: 0.8489 - val_loss: 0.3983\n",
      "Epoch 12/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8642 - loss: 0.3763 - val_accuracy: 0.8355 - val_loss: 0.3925\n",
      "Epoch 13/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8673 - loss: 0.3595 - val_accuracy: 0.8459 - val_loss: 0.3773\n",
      "Epoch 14/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8669 - loss: 0.3460 - val_accuracy: 0.8577 - val_loss: 0.3600\n",
      "Epoch 15/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8781 - loss: 0.3258 - val_accuracy: 0.8704 - val_loss: 0.3429\n",
      "Epoch 16/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8894 - loss: 0.3088 - val_accuracy: 0.8713 - val_loss: 0.3349\n",
      "Epoch 17/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8828 - loss: 0.3108 - val_accuracy: 0.8687 - val_loss: 0.3332\n",
      "Epoch 18/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8942 - loss: 0.2914 - val_accuracy: 0.8767 - val_loss: 0.3209\n",
      "Epoch 19/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8981 - loss: 0.2828 - val_accuracy: 0.8596 - val_loss: 0.3335\n",
      "Epoch 20/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9009 - loss: 0.2784 - val_accuracy: 0.8659 - val_loss: 0.3234\n",
      "Epoch 21/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9057 - loss: 0.2688 - val_accuracy: 0.8676 - val_loss: 0.3193\n",
      "Epoch 22/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9040 - loss: 0.2604 - val_accuracy: 0.8829 - val_loss: 0.3024\n",
      "Epoch 23/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9060 - loss: 0.2581 - val_accuracy: 0.8703 - val_loss: 0.3127\n",
      "Epoch 24/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9105 - loss: 0.2480 - val_accuracy: 0.8804 - val_loss: 0.3003\n",
      "Epoch 25/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9101 - loss: 0.2423 - val_accuracy: 0.8860 - val_loss: 0.2932\n",
      "Epoch 26/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9154 - loss: 0.2354 - val_accuracy: 0.8657 - val_loss: 0.3148\n",
      "Epoch 27/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9183 - loss: 0.2291 - val_accuracy: 0.8557 - val_loss: 0.3306\n",
      "Epoch 28/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9138 - loss: 0.2266 - val_accuracy: 0.8879 - val_loss: 0.2870\n",
      "Epoch 29/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9120 - loss: 0.2321 - val_accuracy: 0.8876 - val_loss: 0.2864\n",
      "Epoch 30/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9169 - loss: 0.2206 - val_accuracy: 0.8723 - val_loss: 0.3041\n"
     ]
    }
   ],
   "source": [
    "# Fitting train data and then testing train\n",
    "history = model.fit(train_data, train_labels, epochs=30, batch_size=512, validation_data=(test_data, test_labels))\n",
    "\n",
    "# Accuracy of ~0.87 with current dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step - accuracy: 0.8758 - loss: 0.2915\n"
     ]
    }
   ],
   "source": [
    "# Testing accuracy\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews      <START superb i had initially thought that giv...\n",
      "Sentiment                                             positive\n",
      "Name: 33, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Grabbing random user review sample\n",
    "index = np.random.randint(1, 1000)\n",
    "user_review = test_reviews.loc[index]\n",
    "print(user_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "positive sentiment with probability of 0.6631073\n"
     ]
    }
   ],
   "source": [
    "# Testing it against model\n",
    "user_review = test_data[index]\n",
    "user_review = np.array([user_review])\n",
    "probability = model.predict(user_review)\n",
    "if (probability > 0.5).astype('int32'):\n",
    "    print('positive sentiment with probability of', probability[0][0])\n",
    "else:\n",
    "    print('negative sentiment with probability of', 1.0-probability[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
